1. Which order performs the worst? Why? Please write down the latency captured by time for the worst order.	
| 4 1 | | 1 3 |
| 3 2 | | 2 4 |
   A	   B
The order that performs the worst is JKI with a latency of 17m.349s. This happens because the cache must load and 
constantly replace all values of the first matrix (if we were multiplying matrix A by matrix B to result in matrix C).
For instance, we made up an example using a 2x2 matrix (insert matrix example here). Based off each iteration of the loop, 
the numbers multiplied goes as follows - top left corner (4) of matrix A is multiplied by top left corner (1) of matrix B - this 
is then stored into the top left corner of Matrix C (4). Then, the bottom left corner (3) of matrix A is multiplied by top left corner 
(1) of Matrix B and stored into the bottom left corner of Matrix C (3). Now the “middle loop” is incremented. Now, the top right corner 
(1) of Matrix A is multiplied by bottom left corner (2) of Matrix B and added to the previous value that was stored in the top left of Matrix C (4 + (2*1)). 
Then, the bottom right corner of Matrix B (2) is multiplied by the bottom left corner of Matrix A (2) and added to the bottom left corner of Matrix C (3 + (2*2)).
Now the left side of Matrix C looks like this (put matrix c in here ). This pattern continues. The cache looks for more values in Matrix A resulting in more misses depending on cache size. 

2. Which order performs the best? Why? Please write down the latency captured by time for the best best.
| 4 1 | | 1 3 |
| 3 2 | | 2 4 |
   A	   B
The order that performs the best is IKJ with a latency of 0.463s. This happens because the cache is only loading 
one value from Matrix A and uses this value to compute before moving onto the next value in Matrix B. This ends up 
with less misses and gives us a better hit/ratio, which essentially means it’ll be faster. Using the same matrices mentioned 
above, the top left corner of Matrix A (4) is multiplied by the top left corner of Matrix B (1) and stored in the top left 
corner of Matrix C(4). Now, the top left corner of Matrix A (4, again) is multiplied by the top right corner of Matrix B (3) 
and stored into the top right corner of Matrix C (12). Then, the “middle” loop is incremented and the top right corner of 
Matrix A (1) is multiplied by the bottom left corner of Matrix B (2) and added to the previous value of Matrix C in the top 
left corner (4 + (2*1)). Following this, the top right corner (used again) of Matrix A (1) is multiplied by the bottom right corner of 
Matrix B (4) and added to the previous value stored in the Matrix C (12 + (4*1)). 

3. Does the way we stride through the matrices with respect to the innermost loop affect performance? 

Yes it does affect performance. As mentioned in the instructions of this lab “to compute the matrix multiplication correctly, the loop order 
doesn't matter” although the performance is based on how we are striding or accessing the matrix.The innermost loop is a resulting loop to compute 
the value in the final matrix relying on the previous inner loops (the loops that stride through the matrices). A cache performs better when there 
are more cache hits and less cache misses - better spatial (data you access already ’n’ and predicting the next data accessed will be ‘n+1’) and temporal
locality (if instruction will be executed multiple times, the data you just accessed will be accessed again shortly). 

4. Please complete the following table using valgrind to measure D1 miss rate with regard to different matrix size.
_________________________________________________________________________________________________________________________________________________________________________________
|Cache miss w.r.t matrix size	|1024			|512			|256			|128			|64			|32			|
|-------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|
|matrix_mult_ijk		|33.4% (33.4% + 34.7%)	|33.4% (33.4% +34%)	|33.4%(33.4% + 31.4%)	|32.7% (32.9% + 24.7%)	|1.8%(1.4% + 8.8%)	|4.2% (3.7% + 6.9%)	|
|-------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|
|matrix_mult_jki		|100% (100% + 18.6%)	|99.9% (100% + 18.2%)	|99.7%(99.9% + 16.8%)	|98.5%(99% + 13.5%)	|3.5%(3.3% + 9.4%)	|2.8%(2.3% + 7%)	|
|-------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|
|matrix_mult_ikj		|6.3% (8.3 % + 0.1%)	|6.3% (8.3% + 0.1%)	|6.3% (8.3% + 0.3%)	|6.4%(8.3% + 0.7%)	|1.4%(1.3% + 1.8%)	|3.8% (3.7% + 4.2%)	|
|_______________________________|_______________________|_______________________|_______________________|_______________________|_______________________|_______________________|
	
5. Based on the table of q4, does the size of the matrix affect performance? Why? Have you noticed the cache miss rate dramatically changes upon a certain dim size? What is the size of the matrix when it happens? and why would it happen? 

Yes, the size of the matrix affects performance, as shown in the chart, because the number of iterations increases as “DIM” increases. There is a dramatic change in the cache miss rate when the DIM size changes to 128 because this exceeds the overall size of the cache. 